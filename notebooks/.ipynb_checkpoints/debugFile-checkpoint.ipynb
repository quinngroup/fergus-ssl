{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are labels of known points: [0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/madhura/.local/lib/python2.7/site-packages/IPython/kernel/__main__.py:46: DeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt1\n",
    "import timeit\n",
    "import sys\n",
    "import os\n",
    "from sklearn.cross_validation import KFold\n",
    "from collections import OrderedDict\n",
    "import operator\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import scipy.linalg as LA\n",
    "import scipy.sparse\n",
    "import sklearn.utils.arpack as SLA\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.manifold import spectral_embedding\n",
    "from pyspark.mllib.clustering import GaussianMixture, GaussianMixtureModel\n",
    "import sklearn.metrics.pairwise as pairwise\n",
    "from sklearn import decomposition as pca\n",
    "from scipy import interpolate as ip\n",
    "import sklearn.mixture as mixture\n",
    "import sys\n",
    "from sklearn.metrics.pairwise import chi2_kernel\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "%matplotlib inline\n",
    "\n",
    "dataX,dataY=datasets.make_blobs(n_samples=20000, n_features=10, centers=2, cluster_std=1.5, center_box=(-10.0, 10.0), shuffle=True, random_state=None)\n",
    "\n",
    "def labelremover(X,y):\n",
    "    newX1 = np.around(X,decimals=2)\n",
    "    newY1=np.copy(y)\n",
    "    dim = X.shape[1]\n",
    "    points = np.array(np.empty(len(np.unique(y))))\n",
    "    knownX = np.empty((len(points),dim))\n",
    "    knownY = np.empty(len(points))\n",
    "    for i in np.unique(y):\n",
    "        points[i] = np.where(y==(i))[0][0]\n",
    "    for j in np.arange(0,len(newY1)):\n",
    "        newY1[j]=-1\n",
    "    for k in np.unique(y):\n",
    "        newY1[points[k]] = y[points[k]]\n",
    "    knownX = X[[i for i in points]]\n",
    "    knownY = y[[i for i in points]]\n",
    "    print \"These are labels of known points: \"+ str(knownY)\n",
    "    return (newY1, knownX, knownY)\n",
    "\n",
    "trainX = dataX[0:18000,:]\n",
    "trainY = dataY[0:18000]\n",
    "testX = dataX[18000:20000,:]\n",
    "testY = dataY[18000:20000]\n",
    "\n",
    "\n",
    "newtrainY, knownX, knownY = labelremover(trainX,trainY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2]\n"
     ]
    }
   ],
   "source": [
    "#Standalone code\n",
    "X = trainX\n",
    "y = newtrainY\n",
    "\n",
    "classes = np.sort(np.unique(y))\n",
    "PCA = pca.PCA(n_components=X.shape[1])\n",
    "rotatedData = PCA.fit_transform(X)\n",
    "\n",
    "#plt.scatter(rotatedData[:,0], rotatedData[:,1], c=trainY, cmap = (('ocean')))\n",
    "\n",
    "X_ = rotatedData\n",
    "\n",
    "dim = X_.shape[1]\n",
    "classes = np.delete(classes, 0) # remove the -1 from this list\n",
    "k = np.size(classes)\n",
    "\n",
    "numBins =k+1\n",
    "\n",
    "def _get_kernel(X, y = None,ker=None):\n",
    "    \n",
    "    if ker == \"rbf\":\n",
    "        if y is None:\n",
    "            return pairwise.rbf_kernel(X, X, gamma = 625)\n",
    "        else:\n",
    "            return pairwise.rbf_kernel(X, y, gamma = 625)\n",
    "    elif ker == \"linear\":\n",
    "        \n",
    "        dist = DistanceMetric.get_metric('euclidean')\n",
    "        if y is None:\n",
    "            return pairwise.euclidean_distances(X, X)\n",
    "            #return dist.pairwise(X)\n",
    "        else:\n",
    "            return pairwise.euclidean_distances(X, y)\n",
    "            #return dist.pairwise(X)\n",
    "    else:\n",
    "        raise ValueError(\"is not a valid kernel. Only rbf and euclidean\"\n",
    "                         \" are supported at this time\")\n",
    "\n",
    "\n",
    "sig = np.zeros(dim)\n",
    "gee = np.zeros((numBins,dim))\n",
    "b_edgeMeans = np.zeros((numBins,dim))\n",
    "interpolators = []\n",
    "index = []\n",
    "\n",
    "\n",
    "for i in range(dim):\n",
    "    histograms,binEdges = np.histogram(X_[:,i],bins=numBins,density=True)\n",
    "    #add 0.01 to histograms and normalize it\n",
    "    histograms = histograms+ 0.01\n",
    "    #print histograms\n",
    "    histograms /= histograms.sum()\n",
    "    # calculating means on the bin edges as the x-axis for the interpolators\n",
    "    b_edgeMeans[:,i] = np.array([binEdges[j:j + 2].mean() for j in range(binEdges.shape[0] - 1)])\n",
    "    #print b_edgeMeans\n",
    "    #get D~, P, W~\n",
    "    '''\n",
    "    Wdis = Affinity between discrete points.\n",
    "    Since Affinity matrix will be build on one histogram at a time. I am using pairwise linear- kernel affinities\n",
    "    P  = Diagonal matrix of histograms\n",
    "    Ddis = Diagonal matrix whose diagonal elements are the sum of the columns of PW~P\n",
    "    Dhat = Diagonal matrix whose diagonal elements are the sum of columns of PW~\n",
    "    '''\n",
    "    Wdis = _get_kernel(histograms.reshape(histograms.shape[0],1),y=None,ker=\"linear\")\n",
    "    #print Wdis\n",
    "    P  = np.diag(histograms)\n",
    "    #print P\n",
    "    Ddis = np.diag(np.sum((P.dot(Wdis.dot(P))),axis=0))\n",
    "    #print Ddis\n",
    "    Dhat = np.diag(np.sum(P.dot(Wdis),axis=0))\n",
    "    #print Dhat\n",
    "    #Creating generalized eigenfunctions and eigenvalues from histograms.\n",
    "    sigmaVals, functions = scipy.linalg.eig((Ddis-(P.dot(Wdis.dot(P)))),(P.dot(Dhat)))\n",
    "    #print functions.shape\n",
    "    #print(\"eigenValue\"+repr(i)+\": \"+repr(np.real(sigmaVals[0:self.k]))+\"Eigenfunctions\"+repr(i)+\": \"+repr(np.real(functions[:,0:self.k])))\n",
    "    #print np.sort(np.real(sigmaVals))\n",
    "    arg = np.argsort(np.real(sigmaVals))[1]\n",
    "    index.append(arg)\n",
    "    sig[i] = np.real(sigmaVals)[arg]\n",
    "    gee[:,i] = np.real(functions)[:,arg]\n",
    "    #b_edge_for_test[:,i] = b_edgeMeans[:,arg]\n",
    "    #print \"====\"\n",
    "'''for i in index:\n",
    "    plt.hist(X_[:,i], bins = numBins)\n",
    "    plt.axis([binEdges.min()-5, binEdges.max()+5, 0, X_.shape[0]])\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "if np.isnan(np.min(sig)):\n",
    "    nan_num = np.isnan(sig)\n",
    "    sig[nan_num] = 0\n",
    "newsig = np.zeros(k)\n",
    "newg = np.zeros((numBins,k))\n",
    "newEdgeMeans = np.zeros((numBins,k))\n",
    "transformeddata = np.zeros((X_.shape[0],k))\n",
    "approxValues = np.zeros((X_.shape[0],k))\n",
    "\n",
    "def get_transformed_data(ori_data,edge_means,i):\n",
    "    dim = edge_means.shape[1]\n",
    "    transformeddata = np.empty((ori_data.shape[0],dim))\n",
    "    if ori_data[:,i].min() < edge_means[:,i].min() or ori_data[:,i].max() > edge_means[:,i].max():\n",
    "        ls=[]\n",
    "        for num in ori_data[:,i]:\n",
    "            val = (((edge_means[:,i].max()-edge_means[:,i].min())*(num - ori_data[:,i].min()))/(ori_data[:,i].max() - ori_data[:,i].min())) + edge_means[:,i].min()\n",
    "            if num==ori_data[:,i].min():\n",
    "                val = val + 0.001\n",
    "            if num==ori_data[:,i].max():\n",
    "                val = val - 0.001\n",
    "            ls.append(val)\n",
    "        return np.array(ls)\n",
    "        #transformeddata[i,:] = transformer(b_edgeMeans[i].min(), b_edgeMeans[i].max(), self.X_[:,i])\n",
    "    else:\n",
    "        print(\"within range\")\n",
    "        return ori_data[:,i]\n",
    "#selecting the first k eigenvalues and corresponding eigenvectors from all dimensions\n",
    "\n",
    "ind =  np.argsort(sig)[0:k]\n",
    "print ind\n",
    "newsig = sig[ind]\n",
    "newg = gee[:,ind]\n",
    "newEdgeMeans = b_edgeMeans[:,ind]\n",
    "allinterpolators = []\n",
    "b_edge_for_test = np.zeros((numBins,dim))\n",
    "for i in range(0,k):\n",
    "    interpolators.append(ip.interp1d(newEdgeMeans[:,i], newg[:,i]))\n",
    "    transformeddata[:,i] = get_transformed_data(X_,newEdgeMeans,i)\n",
    "    approxValues[:,i] = interpolators[i](transformeddata[:,i])\n",
    "\n",
    "U = approxValues\n",
    "S = np.diag(newsig)\n",
    "V = np.diag(np.zeros(np.size(y)))\n",
    "\n",
    "labeled = np.where(y != -1)\n",
    "V[labeled, labeled] = 10\n",
    "\n",
    "# Solve for alpha and use it to compute the eigenfunctions, f.\n",
    "A = S + np.dot(np.dot(U.T, V), U)\n",
    "if np.linalg.det(A) == 0:\n",
    "    A = A + np.eye(A.shape[1])*0.000001\n",
    "b = np.dot(np.dot(U.T, V), y)\n",
    "#print \"this is A: \"+ str(A)\n",
    "#print \"this is b: \"+ str(b)\n",
    "alpha = np.linalg.solve(A, b)\n",
    "\n",
    "#labels_ = solver(U, alpha)\n",
    "f = np.dot(U,alpha)\n",
    "f = f.reshape((f.shape[0],-1))\n",
    "# Set up a GMM to assign the hard labels from the eigenfunctions.\n",
    "g = mixture.GMM(n_components = np.size(classes),n_iter=5000, covariance_type='full',min_covar=0.0000001)\n",
    "g.fit(f)\n",
    "labels_ = g.predict(f)\n",
    "\n",
    "means = np.argsort(g.means_.flatten())\n",
    "\n",
    "for i in range(0, np.size(labels_)):\n",
    "    labels_[i] = np.where(means == labels_[i])[0][0]\n",
    "\n",
    "#===========================================================================\n",
    "#predict\n",
    "tx = PCA.transform(testX)\n",
    "approxVec1 = np.zeros((tx.shape[0],k))\n",
    "allpoints = np.zeros((tx.shape[0],k))\n",
    "for i in range(k):\n",
    "    allpoints[:,i] = get_transformed_data(tx[:,0:k], newEdgeMeans,i)\n",
    "#print allpoints\n",
    "for p in range(tx.shape[0]):\n",
    "    kpoints = allpoints[p]\n",
    "    #print kpoints\n",
    "    for d in range(len(kpoints)):\n",
    "        val = kpoints[d]\n",
    "        approxVec1[p,d] = interpolators[d](val)\n",
    "\n",
    "\n",
    "tf = np.dot(approxVec1,alpha)\n",
    "tf = tf.reshape((tf.shape[0],-1))\n",
    "tlabels_ = g.predict(tf)\n",
    "for i in range(0, np.size(tlabels_)):\n",
    "    tlabels_[i] = np.where(means == tlabels_[i])[0][0]\n",
    "\n",
    "#===========================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 575.0 failed 1 times, most recent failure: Lost task 0.0 in stage 575.0 (TID 2192, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/madhura/spark2.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/home/madhura/spark2.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 2096, in <lambda>\n    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 2096, in <genexpr>\n    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()\n  File \"<ipython-input-20-f7c1adb59395>\", line 176, in <lambda>\nTypeError: transformer() takes exactly 3 arguments (2 given)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:893)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:892)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor108.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/madhura/spark2.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/home/madhura/spark2.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 2096, in <lambda>\n    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 2096, in <genexpr>\n    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()\n  File \"<ipython-input-20-f7c1adb59395>\", line 176, in <lambda>\nTypeError: transformer() takes exactly 3 arguments (2 given)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-1efd6be7687a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mlpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplabels_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-f7c1adb59395>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# S: Diagonal matrix of k smallest eigenvalues. (k by k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# V: Diagonal matrix of LaGrange multipliers for labeled data, 0 for unlabeled. (n_samples by n_samples)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindRowMatMaker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapproxValues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mlabeled_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzipWithIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0mmatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-f7c1adb59395>\u001b[0m in \u001b[0;36mindRowMatMaker\u001b[0;34m(self, irm)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mindRowMatMaker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mirm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndexedRowMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mirm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzipWithIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mIndexedRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/madhura/spark2.0/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mzipWithIndex\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2094\u001b[0m         \u001b[0mstarts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2096\u001b[0;31m             \u001b[0mnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2097\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m                 \u001b[0mstarts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/madhura/spark2.0/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \"\"\"\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/madhura/spark2.0/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/madhura/spark2.0/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/madhura/spark2.0/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 575.0 failed 1 times, most recent failure: Lost task 0.0 in stage 575.0 (TID 2192, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/madhura/spark2.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/home/madhura/spark2.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 2096, in <lambda>\n    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 2096, in <genexpr>\n    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()\n  File \"<ipython-input-20-f7c1adb59395>\", line 176, in <lambda>\nTypeError: transformer() takes exactly 3 arguments (2 given)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:893)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:892)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor108.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/madhura/spark2.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/home/madhura/spark2.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 317, in func\n    return f(iterator)\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 2096, in <lambda>\n    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()\n  File \"/home/madhura/spark2.0/python/pyspark/rdd.py\", line 2096, in <genexpr>\n    nums = self.mapPartitions(lambda it: [sum(1 for i in it)]).collect()\n  File \"<ipython-input-20-f7c1adb59395>\", line 176, in <lambda>\nTypeError: transformer() takes exactly 3 arguments (2 given)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#Distributed code\n",
    "\n",
    "%run LabelPropagationDistributed.ipynb\n",
    "\n",
    "lpd = LabelPropagationDistributed(numBins=3)\n",
    "\n",
    "dX = sc.parallelize(trainX)\n",
    "dy = sc.parallelize(newtrainY)\n",
    "lpd.fit(dX,dy)\n",
    "\n",
    "plabels_ = lpd.predict(sc.parallelize(testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 2.0.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
