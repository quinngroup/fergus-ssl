{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import PCA as PCAmllib\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from numpy.core.numeric import array\n",
    "from scipy.linalg import eig\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, RowMatrix, IndexedRowMatrix\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "from pyspark.sql.types import *\n",
    "from sklearn.metrics import pairwise\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from pyspark.mllib.clustering import GaussianMixture\n",
    "class LabelPropagationDistributed():\n",
    "    '''\n",
    "    The Fergus et al 2009 eigenfunction propagation classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : integer > 0\n",
    "        Number of eigenvectors to use (default: # of labels, not including\n",
    "        unlabeled data) in eigen-decomposition.\n",
    "    lagrangian : float > 0\n",
    "        Lagrangian multiplier to constrain the regularization penalty (default: 10).\n",
    "    numBins : float > 0\n",
    "        Number of bins for the 1-D histograms of data.\n",
    "\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    TODO\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Rob Fergus, Yair Weiss, Antonio Torralba. Semi-Supervised Learning in\n",
    "    Gigantic Image Collections (2009).\n",
    "    http://eprints.pascal-network.org/archive/00005636/01/ssl-1.pdf\n",
    "    '''\n",
    "    global transformer\n",
    "    global selectValues\n",
    "    global bc_EdgeMeans\n",
    "    global bc_newg\n",
    "    global kb\n",
    "    def __init__(self, k = -1,numBins = -1 ,lagrangian = 10):\n",
    "        \n",
    "        self.k = k\n",
    "        self.numBins = numBins\n",
    "        self.lagrangian = lagrangian\n",
    "        \n",
    "    def makeDF(self,rotatedData, dimensions):\n",
    "        X_ = rotatedData.map(lambda vec: np.array(vec))\n",
    "        dataAsDict = X_.map(lambda x: tuple(float(f) for f in x))\n",
    "        schemaString = \"\"\n",
    "        for i in range(dimensions):\n",
    "            i = i+1\n",
    "            schemaString += str(i) + \" \"\n",
    "        schemaString = schemaString.strip()\n",
    "        fields = [StructField(field_name,FloatType(), True) for field_name in schemaString.split()]\n",
    "        schema = StructType(fields)\n",
    "        return sqlContext.createDataFrame(dataAsDict, schema)\n",
    "\n",
    "    def getdataboundaries(self,dictData,k):\n",
    "        dataBounds = OrderedDict()\n",
    "        for i in range(0,k):\n",
    "            s = str(i+1)\n",
    "            tmprdd = dictData.select(s).rdd.map(lambda row: row.asDict().values()[0])\n",
    "            dataBounds[i] = (tmprdd.min(),tmprdd.max())\n",
    "        return dataBounds\n",
    " \n",
    "    \n",
    "    def transformer(vec, bounds, bc_EdgeMeans, bc_newg):\n",
    "        vec1 = vec.toArray()\n",
    "        tmpArr = np.zeros(vec1.shape)\n",
    "        edge_means = bc_EdgeMeans.value\n",
    "        for i in range(len(vec1)):\n",
    "            inter2 = ip.interp1d(edge_means[:,i], bc_newg.value[:,i])\n",
    "            (minVal,maxVal) = bounds[i]\n",
    "            if (minVal < edge_means[:,i].min()) or (maxVal > edge_means[:,i].max()):\n",
    "                val = (((edge_means[:,i].max()-edge_means[:,i].min())*(vec1[i] - minVal))/(maxVal - minVal)) + edge_means[:,i].min()\n",
    "                if vec1[i]==minVal:\n",
    "                    val = val + 0.001\n",
    "                if vec1[i]==maxVal:\n",
    "                    val = val - 0.001\n",
    "            else:\n",
    "                val = vec1[i]\n",
    "            tmpArr[i] = inter2(val)\n",
    "        return DenseVector(tmpArr)\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def indRowMatMaker(irm):\n",
    "        return IndexedRowMatrix(irm.zipWithIndex().map(lambda x:IndexedRow(x[1],x[0])))\n",
    "    \n",
    "    def solver(self,vectors):\n",
    "        U = vectors\n",
    "        f = U.multiply(self.alphablk)\n",
    "        #f = f.reshape((f.shape[0],-1))\n",
    "        return f\n",
    "    \n",
    "    def selectValues(ddict,kb):\n",
    "        desired_keys = sorted([int(k) for k in ddict.keys()])[0:kb.value]\n",
    "        newddict = { i: ddict[str(i)] for i in desired_keys }\n",
    "        return newddict\n",
    "\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        '''\n",
    "        X and y should be RDDs\n",
    "        \n",
    "        Solve for alpha\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        def _get_kernel(X, y = None,ker=None):\n",
    "\n",
    "            if ker == \"rbf\":\n",
    "                if y is None:\n",
    "                    return pairwise.rbf_kernel(X, X, gamma = 625)\n",
    "                else:\n",
    "                    return pairwise.rbf_kernel(X, y, gamma = 625)\n",
    "            elif ker == \"linear\":\n",
    "\n",
    "                #dist = DistanceMetric.get_metric('euclidean')\n",
    "                if y is None:\n",
    "                    return pairwise.euclidean_distances(X, X)\n",
    "                    #return dist.pairwise(X)\n",
    "                else:\n",
    "                    return pairwise.euclidean_distances(X, y)\n",
    "                    #return dist.pairwise(X)\n",
    "            else:\n",
    "                raise ValueError(\"is not a valid kernel. Only rbf and euclidean\"\n",
    "                                 \" are supported at this time\")\n",
    "        \n",
    "        self.newX = X.zipWithIndex().map(lambda (x,y): (y,x))\n",
    "        self.n = X.cache().count()\n",
    "        self.dimensions= X.zipWithIndex().filter(lambda (ls,i): i==0).map(lambda (row,i): len(row)).collect()[0]\n",
    "        self.classes = sorted(y.map(lambda x: (x,1)).reduceByKey(lambda a,b: a+b).map(lambda (x,arr): x).collect())\n",
    "        if self.classes[0] == -1:\n",
    "            self.classes = np.delete(self.classes, 0) # remove the -1 from this list\n",
    "            if self.k == -1:\n",
    "                self.k = np.size(self.classes)\n",
    "        if self.numBins == -1:\n",
    "                self.numBins = self.k\n",
    "        kb = sc.broadcast(self.k)        \n",
    "        sig = np.zeros(self.dimensions)\n",
    "        gee = np.zeros((self.numBins,self.dimensions))\n",
    "        b_edgeMeans = np.zeros((self.numBins,self.dimensions))\n",
    "        #-----------End of declarations-----------------\n",
    "        XforPCA = X.map(lambda rw: Vectors.dense(rw))\n",
    "        self.PCA = PCAmllib(self.dimensions).fit(XforPCA)\n",
    "        rotatedData = self.PCA.transform(XforPCA)\n",
    "        dictData = self.makeDF(rotatedData, self.dimensions)\n",
    "    \n",
    "        for i in range(self.dimensions):\n",
    "            s = str(i+1)\n",
    "            dimRdd = dictData.select(s)\n",
    "            binEdges,histograms = dimRdd.rdd.map(lambda x: x.asDict().values()[0]).histogram(self.numBins)\n",
    "            histograms = array(histograms)\n",
    "            binEdges = np.array(binEdges)\n",
    "            db = array(np.diff(binEdges),float)\n",
    "            histograms = histograms/db/histograms.sum()\n",
    "            histograms = histograms + 0.01\n",
    "            histograms /= histograms.sum()\n",
    "            self.ori_edgem = binEdges\n",
    "            # calculating means on the bin edges as the x-axis for the interpolators\n",
    "            b_edgeMeans[:,i] = np.array([binEdges[j:j + 2].mean() for j in range(binEdges.shape[0] - 1)])\n",
    "            \n",
    "            #get D~, P, W~\n",
    "            '''\n",
    "            Wdis = Affinity between discrete points.\n",
    "            Since Affinity matrix will be build on one histogram at a time. I am using pairwise linear- kernel affinities\n",
    "            P  = Diagonal matrix of histograms\n",
    "            Ddis = Diagonal matrix whose diagonal elements are the sum of the columns of PW~P\n",
    "            Dhat = Diagonal matrix whose diagonal elements are the sum of columns of PW~\n",
    "            '''\n",
    "            Wdis = _get_kernel(histograms.reshape(histograms.shape[0],1),y=None,ker=\"linear\")\n",
    "            P = np.diag(histograms)\n",
    "            Ddis = np.diag(np.sum((P.dot(Wdis.dot(P))),axis=0))\n",
    "            Dhat = np.diag(np.sum(P.dot(Wdis),axis=0))\n",
    "            #Creating generalized eigenfunctions and eigenvalues from histograms.\n",
    "            sigmaVals, functions = eig((Ddis-(P.dot(Wdis.dot(P)))),(P.dot(Dhat)))\n",
    "            arg = np.argsort(np.real(sigmaVals))[1]\n",
    "            sig[i] = np.real(sigmaVals)[arg]\n",
    "            gee[:,i] = np.real(functions)[:,arg]\n",
    "        \n",
    "        # k smallest eigenvalues and eigenvectors\n",
    "        if np.isnan(np.min(sig)):\n",
    "            nan_num = np.isnan(sig)\n",
    "            sig[nan_num] = 0\n",
    "        ind =  np.argsort(sig)[0:self.k]\n",
    "        self.newsig = sig[ind]\n",
    "        self.newg = gee[:,ind]\n",
    "        self.newEdgeMeans = b_edgeMeans[:,ind]\n",
    "        bc_EdgeMeans = sc.broadcast(self.newEdgeMeans)\n",
    "        bc_newg = sc.broadcast(self.newg)\n",
    "        dataBounds = self.getdataboundaries(dictData, self.k)\n",
    "        #b_bounds = sc.broadcast(dataBounds)\n",
    "        \n",
    "        #make it a matrix\n",
    "        makeItMatrix = RowMatrix(dictData.rdd.map(lambda row: selectValues(row.asDict(), kb).values()))\n",
    "        approxValues = makeItMatrix.rows.map(lambda rw: transformer(rw, dataBounds, bc_EdgeMeans, bc_newg))\n",
    "        # U: k eigenvectors corresponding to smallest eigenvalues. (n_samples by k)\n",
    "        # S: Diagonal matrix of k smallest eigenvalues. (k by k)\n",
    "        # V: Diagonal matrix of LaGrange multipliers for labeled data, 0 for unlabeled. (n_samples by n_samples)\n",
    "        U = LabelPropagationDistributed.indRowMatMaker(approxValues)\n",
    "        labeled_ind = np.array(y.zipWithIndex().filter(lambda (a,b): a!=-1).map(lambda (a,b): b).collect())\n",
    "        matent = []\n",
    "        for i in labeled_ind:\n",
    "            matent.append(MatrixEntry(i,i,self.lagrangian))\n",
    "        V = CoordinateMatrix(sc.parallelize(matent),numRows=self.n, numCols=self.n)\n",
    "        Utrans = U.toCoordinateMatrix().transpose()  \n",
    "        Ublk = U.toBlockMatrix()\n",
    "        product1 = Utrans.toBlockMatrix().multiply(V.toBlockMatrix())\n",
    "        product2 = product1.multiply(Ublk)\n",
    "        S = np.diag(newsig)\n",
    "        localblk = product2.toLocalMatrix().toArray()\n",
    "        A = S + localblk\n",
    "        if np.linalg.det(A) == 0:\n",
    "            A = A + np.eye(A.shape[1])*0.000001\n",
    "        yblk = CoordinateMatrix(y.zipWithIndex().map(lambda x: MatrixEntry(x[1],0,x[0]))).toBlockMatrix()\n",
    "        b = product1.multiply(yblk)\n",
    "        b = b.toLocalMatrix().toArray()\n",
    "        alpha = np.linalg.solve(A, b)\n",
    "        self.alphablk = LabelPropagationDistributed.indRowMatMaker(sc.parallelize(alpha)).toBlockMatrix()\n",
    "        efunctions = self.solver(Ublk)\n",
    "        #indec = efunctions.toIndexedRowMatrix().rows.map(lambda row: (row.index,row.vector.toArray()))\n",
    "        ec = efunctions.toCoordinateMatrix().toRowMatrix().rows.map(lambda x: x.toArray())\n",
    "        ec1 = efunctions.blocks.map(lambda (a,b): b.toArray()).flatMap(lambda vec: array(vec))\n",
    "        ec1 = ec1.zipWithIndex().map(lambda x: (x[1],x[0])).sortByKey().map(lambda (a,b): b)\n",
    "        # Set up a GMM to assign the hard labels from the eigenfunctions.\n",
    "        self.gmm = GaussianMixture.train(ec1, np.size(self.classes), convergenceTol=0.0000001,\n",
    "                                         maxIterations=200000, seed=None)\n",
    "        labels_ = self.gmm.predict(ec1)\n",
    "        gaussians = np.zeros((2,1))\n",
    "        i=0\n",
    "        for ls in gm.gaussians:\n",
    "            gaussians[i] = (ls.mu)\n",
    "            i = i +1\n",
    "        distmeans = sc.broadcast(np.argsort(gaussians.flatten()))\n",
    "        self.labels_ = labels_.map(lambda x: np.where(distmeans.value == x)[0][0])\n",
    "        self.newy = self.labels_.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "        self.joined = self.newX.join(self.newy).map(lambda (i,(a,b)): (a,b))\n",
    "        zippedec1 = ec1.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "        self.joined1 = zippedec1.join(self.newy).join(self.newX).map(lambda (i,((a,b),c)): (a,b,c))\n",
    "        #===============================\n",
    "        self.ind = ind\n",
    "        self.U1 = Ublk\n",
    "        self.S = S\n",
    "        self.V = V\n",
    "        self.A1 = A\n",
    "        self.b1 = b\n",
    "        self.b_edgeMeans = b_edgeMeans\n",
    "        self.alphab = self.alphablk\n",
    "        self.ab1 = alpha\n",
    "        self.ec = ec1\n",
    "        self.aprval = approxValues\n",
    "        self.databeforeapproxval = makeItMatrix\n",
    "        self.pcadata = dictData\n",
    "        #=================================\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X,y=None):\n",
    "        '''\n",
    "        X and y should be RDDs\n",
    "        \n",
    "        '''\n",
    "        #testXrdd = sc.parallelize(testX)\n",
    "        testXforPCA = X.map(lambda rw: Vectors.dense(rw))\n",
    "        newX = self.PCA.transform(testXforPCA)\n",
    "        testdf = self.makeDF(newX, self.dimensions)\n",
    "        testdatabounds = self.getdataboundaries(testdf, self.k)\n",
    "        test_bounds = sc.broadcast(testdatabounds)\n",
    "        testmakeItMatrix = RowMatrix(testdf.rdd.map(lambda row: selectValues(row.asDict(), kb).values()))\n",
    "        testapproxValues = testmakeItMatrix.rows.map(lambda rw: transformer(rw, testdatabounds,bc_EdgeMeans, bc_newg))\n",
    "        testapproxValuesblk = LabelPropagationDistributed.indRowMatMaker(testapproxValues).toBlockMatrix()\n",
    "        testfunctions = self.solver(testapproxValuesblk)\n",
    "        testfunc = testfunctions.toCoordinateMatrix().toRowMatrix().rows.map(lambda x: x.toArray())\n",
    "        predictedlabels = gmm.predict(testfunc)\n",
    "        return predictedlabels\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 2.0.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
